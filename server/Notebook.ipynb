{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit threads API\n",
    "Install PRAW `(pip install praw)`. In your backend code, initialize PRAW with your credentials:\n",
    "- PRAW requires client_id, client_secret, and user_agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Env variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "\n",
    "REDDIT_CLIENTID = os.getenv('REDDIT_CLIENTID')\n",
    "REDDIT_SECRET = os.getenv('REDDIT_SECRET')\n",
    "REDDIT_USER_AGENT = os.getenv('REDDIT_USER_AGENT')\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENTID,\n",
    "    client_secret=REDDIT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT,\n",
    ")\n",
    "\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​Once initialized (in read-only mode), you can fetch submissions. For example, to get the latest posts from r/wallstreetbets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
    "posts = subreddit.search(\"TSLA\", time_filter=\"week\", limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply `reddit.subreddit(\"stocks\").hot(limit=20)`​. This yields Submission objects from which you can extract titles, selftext, and comments.\n",
    "\n",
    "Within your API endpoint (or a helper function), query relevant subreddits for the given stock ticker or company name. You might search multiple subreddits (e.g. r/stocks, r/investing) and combine results. For each submission, gather its title and top comments (e.g. use submission.comments). Filter out removed or very short comments. Collect these texts into a list for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon IO API\n",
    "Install polygon-api-client `pip install -U polygon-api-client`. Use it to get historical price data for the ticker. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agg(open=170.29, high=171.92, low=168.82, close=168.82, volume=53682486.0, vwap=170.1029, timestamp=1712203200000, transactions=630783, otc=None), Agg(open=169.59, high=170.39, low=168.95, close=169.58, volume=42104826.0, vwap=169.6415, timestamp=1712289600000, transactions=540854, otc=None), Agg(open=169.03, high=169.2, low=168.24, close=168.45, volume=37425513.0, vwap=168.6637, timestamp=1712548800000, transactions=549987, otc=None), Agg(open=168.7, high=170.08, low=168.35, close=169.67, volume=42451209.0, vwap=169.1566, timestamp=1712635200000, transactions=541699, otc=None), Agg(open=168.8, high=169.09, low=167.11, close=167.78, volume=49691936.0, vwap=167.9914, timestamp=1712721600000, transactions=647587, otc=None)]\n"
     ]
    }
   ],
   "source": [
    "from polygon import RESTClient\n",
    "\n",
    "POLYGON_API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "\n",
    "client = RESTClient(POLYGON_API_KEY)\n",
    "\n",
    "aggs = client.get_aggs(\n",
    "    \"AAPL\",\n",
    "    1,\n",
    "    \"day\",\n",
    "    \"2024-04-04\",\n",
    "    \"2024-04-10\",\n",
    ")\n",
    "print(aggs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "We want an NLG summary, not just labels. A good approach is to use a transformer summarization pipeline (abstractive summarization). Install OpenAI `(pip install openai)`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment towards TSLA appears mixed. While some people recognize that the stock has been down significantly, others feel positively about its current state. There's also concern or interest about Elon Musk selling TSLA shares, which may be impacting perspectives.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"Please provide a natural language summary of the current sentiment towards TSLA based on the discussions.\",\n",
    "    input=\"TSLA has been down a lot. TSLA is great right now. Elon Musk is selling TSLA\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
